---
title: "2024 XR 디바이스 콘텐츠 메이커톤"
slug: "xr-makerthon"
period: "2024.11.22"
tech: ["Unity", "C#", "MetaCore", "MRTK", "ARFoundation", "WebRTC", "Go"]
images: ["https://eaalkymxyfskjojh.public.blob.vercel-storage.com/portfolio/xr_makerthon/thumbnail.png"]
summary: "Open AI와 yolo v11을 활용한 AR 인공지능 요리 어시스턴트"
githubUrl: ""
siteUrl: ""
category: "hackathon"
---
&nbsp;

# XREAL, 최고의 행운

올해, 처음으로 "소프트웨어" 관련 활동을 도전하려고 들어갔던 나의 첫 학회 **XREAL**.
**XREAL**은 `서울대학교 기반 연합 메타버스 학회`로, 내가 하고자 했던 '게임'이라는 방향과 사뭇 달랐다.
난 단순히 유튜브 메타버스 콘텐츠 등을 통해 VR에 관심이 있던 정도였고...
이렇게 메타버스에 진심인 사람들이 많이 있을 거라곤 상상도 못했었다.
심지어 나는 VR기기도 없었고, 개발조차 해본 적 없었다.
어떻게 뽑힌 건지도 의문인 상황이었는데, 나의 간절함이 닿은 걸까..?
아무튼, 나에게 있어 정말 큰 행운이었다. 

활동을 하면서 "아, 나는 어떻게 여기에 있는걸까? 이 대단한 사람들 사이에 내가 있는 게 맞나?"라는 생각이 정말 많이 들었는데,
스스로를 여러 번 돌아본 만큼, 본인이 공부해나가야 할 방향성도 어느 정도 잡아갈 수 있었다. 
그리고 아래에서 상술할 XR 디바이스 메이커톤은 나에게 있어 큰 반환점이 된다.

> “Success is not final, failure is not fatal: It is the courage to continue that counts.”  
> – Winston Churchill

&nbsp;

# 11월 한 달 간 달려왔던 일이 막을 내렸다. 

XR 디바이스 메이커톤은 **2024 XREAL SUMMER HACKATHON**에서 만난 팀원들과 함께 참가했다.
해커톤 당시의 주제는 '가사 편의성을 위한 XR 콘텐츠'였는데, 우리 팀은 <레시피를 추천해주는 요리 어시스턴트>를 제작했고
운이 좋게도 우수상을 받았었다. 

![2024 xreal 해커톤](https://eaalkymxyfskjojh.public.blob.vercel-storage.com/portfolio/xr_makerthon/1-1.png)

우리 팀은 이 주제를 고도화해서 메이커톤에 출품하고자 했고,
최종적으로 <AR 레시피 추천 인공지능 요리 어시스턴트>를 제작했다. 

![해커톤](https://eaalkymxyfskjojh.public.blob.vercel-storage.com/portfolio/xr_makerthon/1.png)

최종적으로 완성한 서비스는 다음과 같은 플로우를 갖고 있다.

- AR 글라스를 착용한 사용자가 냉장고 문을 열면  
- 글라스의 카메라가 냉장고 속 식재료를 인식하고  
- 이를 바탕으로 인공지능이 적절한 레시피를 추천  
- 추천받은 레시피를 선택하면, AR UI와 음성 안내를 통해 요리를 시작

기획은 심플하지만 이를 구현하기 위해 다양한 기술을 사용해야 했고,
실제 시연 당시 "너무 많은 기능을 담다 보니 앱이 무거워졌다"는 피드백을 받았다.

&nbsp;

# 기술 스택 및 구조

### 1. 컴퓨터 비전 (YOLO v11)
- AR 글라스에서 서버로 영상 스트림 전송
- YOLO v11 모델을 통해 식재료 인식 후 결과 반환

### 2. 실시간 음성 상호작용 (OpenAI Realtime API)
- WebRTC로 음성 데이터를 서버에 실시간 전송
- OpenAI API로 음성→텍스트 변환 및 명령 처리

### 3. GPT 프롬프트 엔지니어링
- AI가 요리 맥락에 집중하도록 지시문 세부 설계

### 4. WebRTC & P2P 통신
- AR 글라스 ↔ 서버 간 음성/비디오/데이터 송수신
- Unity WebRTC + WebSocketSharp를 활용한 실시간 연결 구성

&nbsp;

# 챌린지

이번 대회를 통해 내가 경험한 것을 요약해보자면... 처음부터 끝까지 도전과 실패의 연속이었지만,
그 속에서 더 많이 배우고 성장할 수 있었던 값진 시간이었다.
비록 수상을 하진 못했지만, 본선을 준비하는 과정에서 얻었던 경험 또한 수상만큼 값진 것들이었다.
이 실패의 경험이 시작의 발판이 되어 언젠가 성장한 모습으로 돌아와 다시 한 번 도전할 것이다.

나는 이번 프로젝트에서 서버와 WebRTC 통신을 하는 클라이언트 구현과 Unity를 활용한 AR 글라스 화면 구성을 맡았다.
부끄럽지만, 프로젝트를 시작할 때 서버와 클라이언트의 관계도 제대로 몰랐고, WebSocket과 WebRTC는 그때 처음 접했다. 

그리고 또 하나, **시장에 출시되지 않은 XR 디바이스**로 개발해야 한다는 점은 메타퀘스트 개발도 익숙하지 않은 나에게 큰 도전이었다.

이번 메이커톤은 피앤씨솔루션이라는 기업의 프로토타입 기기인 '메타렌즈2'를 기반으로 진행한 대회였다.
당시 메타렌즈2는 공개된 레퍼런스도 없고, 커뮤니티나 튜토리얼도 전무했던 생소한 기기였다.
한 번도 다뤄본 적 없는 하드웨어 위에서 모두 처음부터 개발해야 했기에 상당한 부담감과 긴장감을 느꼈다.

&nbsp;

## 모르는 개념은 차근차근 배우면서

WebRTC를 구현하려면 개념부터 잡아야 했기에,
가장 먼저 html과 js로 간단한 화상 채팅 기능을 만들어보며 기초를 다졌다.
그런 다음 Unity에서 C#으로 WebRTC를 구현했는데, 이 과정에서 WebSocketSharp 라이브러리와 Unity WebRTC 패키지를 사용했다.

가장 어려웠던 점은 각 라이브러리의 API를 분석하고 이해하는 것이었다.
문서 하나하나를 꼼꼼히 읽고, 기능을 테스트하면서 C# 문법과 API 활용 능력을 꾸준히 키워나갔다.
이번 프로젝트를 통해 이론과 실습을 동시에 익힌 셈이었다.
그냥 무작정 들이대면 된다.
결국 마지막엔 다 해내더라!

&nbsp;

# 메이커톤인데 해커톤을 하고 있었던 건에 대하여...

WebRTC 구현 과정에서 AR 글라스에서 서버로 비디오 스트림을 전송하는 데 문제가 생겼다.
전송된 영상이 화면에 제대로 표시되지 않고 회색 텍스처로만 출력된 것이다.

이를 해결하기 위해 성멘님과 함께 새벽 3시부터 디스코드에서 VS Code Live Share로 협업하며 코드를 수정했고, 아침 10시가 되어서야 문제를 해결했다. 
이후에도 렉 문제가 심각했지만, 프레임 해상도를 낮춰 전송하는 방식으로 최적화했다.

&nbsp;

## 작동이 안 됐던 주요 가능성

### RenderTexture 포맷 호환성 문제
- WebRTC에서 사용하는 RenderTexture는 GPU와 CPU 간 데이터 전송의 핵심이다.
- 기존 코드에서는 rtcVideoTexture가 초기화되지 않았거나, WebRTC에서 지원하지 않는 포맷이 지정되어 VideoStreamTrack이 올바르게 작동하지 않았을 가능성이 있다.
- 수정된 코드에서는 `WebRTC.GetSupportedRenderTextureFormat(SystemInfo.graphicsDeviceType)`을 통해  
호환 가능한 포맷을 동적으로 받아 오고, 해당 포맷으로 RenderTexture를 생성해 연결했더니 이후 영상이 정상적으로 출력되었다.

### AR 텍스처 → WebRTC 전송 흐름 불일치
- ARFoundation에서 제공하는 카메라 텍스처는 GPU에서 처리되는 렌더 텍스처이기 때문에,  
이를 WebRTC로 전송하려면 CPU에서 접근 가능한 텍스처로의 복사 과정이 필요하다.  
기존 코드에서는 이 전환 과정이 명확하지 않았고, 그로 인해 회색 화면만 출력되었을 가능성이 있다.  
- 이를 해결하기 위해 AR 텍스처를 3배 다운샘플링한 후 `Graphics.Blit()`으로 RenderTexture에 복사하여 전송했고,  
이후 성능과 시각 출력 모두 개선되었다.

### XRCpuImage 데이터 처리 누락
- ARFoundation의 `ARCameraManager`에서 제공하는 `XRCpuImage`는  
카메라의 CPU 접근 가능한 이미지 버퍼를 직접 다룰 수 있게 해주지만,  
초기에는 이 데이터를 Texture2D로 변환하거나 RenderTexture로 복사하는 처리가 누락되어 있었다.  
- 최종 코드에서는 `XRCpuImage`를 `Convert` 메서드를 통해 RGBA 포맷의 Texture2D로 변환한 후,  
해당 텍스처를 RenderTexture로 복사해 WebRTC로 전송하면서 문제를 해결했다.

서버에서 정상 화면을 받은 순간... 둘은 소리를 질렀고 일요일 오전 10시, 잠에 들었다.

&nbsp;

# 협업의 힘

이 프로젝트에서 가장 감사한 점은 훌륭한 팀원들과 함께했다는 것이다.
특히 백엔드를 담당한 성멘 님의 도움 덕분에 복잡한 서버 처리를 클라이언트 입장에서 단순하게 호출만 해도 되도록 구조화할 수 있었다. 
항상 의논에 성실히 응해주시고, 새벽에도 함께 디버깅해주신 점은 정말 잊지 못할 감사한 경험이었다.

대회 하루 전, 우리는 모두 모여 간이 해커톤을 진행했다.
여름 해커톤을 다시 생각나게 하는 날이었다.
즐겁다.
나는 그냥 좋은 사람들이랑 함께 작업하는 게 좋더라...

&nbsp;

# 심사 피드백 복기

### 🧑‍🦰 심사위원 1 – 기술적 완성도 지적

- 서버 의존도를 지적하며 "디바이스 성능만으로 구현했어야 했다"는 피드백
- AR 글라스 초점 문제, UI 렌더링 문제 등 다소 억울한 지적도 있었음
    - 메타렌즈 자체가 애플 비전프로처럼 사용자에게 초점을 맞춰주는 프로세스가 있는 기기가 아니다.
    그냥 사용자가 기기를 움직여서 초점이 가장 잘 맞는 위치에서 고정시키는 방식이다.

&nbsp;

### 🧑‍🦳 심사위원 2 – 기획 및 구현력 칭찬

- 과거 삼성의 스마트 냉장고 프로젝트와 유사하다며 높은 관심을 보이심
- 대학생 팀으로서 대기업 수준의 프로토타입을 구현했다는 점을 긍정적으로 평가
- "내가 돈을 주고 사용할 만한 서비스인가?"라는 관점에서 시장성 보완을 조언
- 해외 편의점의 실시간 재고 관리 시스템 등 참고 사례 제시

&nbsp;

# 시연에서의 아쉬움

- **발열로 인한 디바이스 렉**  
  메타렌즈 발열로 시연 초반 카메라 인식이 지연됨 → 기능을 충분히 보여드리지 못함
- **음성 인식 실패**  
  소란스러운 시연 환경에서 OpenAI 음성 API가 제대로 작동하지 않음
- **UI 배치 오류**  
  팔로우 UI가 시야를 가리는 문제 발생  
  → 고정 UI 또는 Object Manipulator 기능을 사용했다면 회피 가능했을 문제
- **버튼 버그 미해결**  
  특정 버튼은 잘 눌렸지만, 이상하게 몇몇 버튼이 제대로 눌리지 않는 문제가 있었다.
  음성 상호작용이 잘 안 되는 상황에서는 버튼이 대체 수단으로 작동했어야 했는데, 이 문제를 끝내 고치지 못했다.
  버튼 버그를 잡았어야 했다. 이건 완전히 내 실책이고, 지금까지도 큰 아쉬움으로 남는다. 팀원들에게 너무나도 미안했다.

이 모든 문제는 결국 완성도가 조금만 더 높았더라면 충분히 피할 수 있는 부분들이었다.
특히, 시연 환경과 디바이스 특성을 더 깊이 고민하지 못했던 점이 아쉽다.

&nbsp;

# 돌아보며

이번 메이커톤은 나에게 있어 단순한 공모전이 아니라, **기술을 배우는 과정, 팀과 협업하는 방법, 한계를 마주하고 고민하는 훈련**이었다.

물론 수상하지는 못했지만, 저는 그보다 더 소중한 것을 얻었다고 생각한다. 

그리고 무엇보다도, 함께한 팀원들, 특히 성멘님 덕분에 나 혼자였다면 결코 완성하지 못했을 프로젝트를 마무리할 수 있었다.

> 다음엔 더 성장한 모습으로, 또다시 도전할 수 있기를.

![요리보](https://eaalkymxyfskjojh.public.blob.vercel-storage.com/portfolio/xr_makerthon/2.png)

*안녕, 요리보*